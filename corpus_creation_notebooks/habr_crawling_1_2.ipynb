{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgFAsxqf55Zg",
        "outputId": "0be5ac18-d105-4a2a-c5f6-0832297ad8f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
            "Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-1.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "import os\n",
        "import json\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "mVEWhfD1IJ-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "def scrape_articles(max_pages=12):\n",
        "    \"\"\"\n",
        "    Функция для получения ссылок статей с сайта https://habr.com/ru/hubs/programming/articles/\n",
        "\n",
        "    Args:\n",
        "        max_pages (int): Максимальное количество страниц для обработки.\n",
        "\n",
        "    Returns:\n",
        "        list: Список ссылок на статьи.\n",
        "    \"\"\"\n",
        "    article_links = []\n",
        "    ua = UserAgent()\n",
        "\n",
        "    for page_num in range(1, max_pages + 1):\n",
        "        url = f'https://habr.com/ru/hubs/programming/articles/page{page_num}/'\n",
        "        session = requests.session()\n",
        "\n",
        "        try:\n",
        "            req = session.get(url, headers={'User-Agent': ua.random})\n",
        "            req.raise_for_status()\n",
        "\n",
        "            page = req.text\n",
        "            soup = BeautifulSoup(page, 'html.parser')\n",
        "\n",
        "            article_blocks = soup.find_all('article', {'class': 'tm-articles-list__item'})\n",
        "            for article_block in article_blocks:\n",
        "                article_tag = article_block.find(\"a\", class_=\"tm-title__link\")\n",
        "                article_link = \"https://habr.com/\" + article_tag[\"href\"]\n",
        "                article_links.append(article_link)\n",
        "\n",
        "        except (requests.exceptions.RequestException, Exception) as e:\n",
        "            print(f\"Error occurred while scraping page {page_num}: {e}\")\n",
        "\n",
        "    return article_links\n"
      ],
      "metadata": {
        "id": "i3ufE88FFaLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "def parse_article_info(article_links):\n",
        "    \"\"\"\n",
        "    Функция для получения информации о статье по ее ссылке и сохранения данных в текстовый файл.\n",
        "\n",
        "    Args:\n",
        "        article_links (list): Список ссылок на статьи.\n",
        "    \"\"\"\n",
        "    ua = UserAgent()\n",
        "\n",
        "    # Создаем папку для сохранения файлов\n",
        "    os.makedirs(\"corpus\", exist_ok=True)\n",
        "\n",
        "    for link in article_links:\n",
        "        session = requests.session()\n",
        "        try:\n",
        "            req = session.get(link, headers={'User-Agent': ua.random})\n",
        "            req.raise_for_status()\n",
        "\n",
        "            page = req.text\n",
        "            soup = BeautifulSoup(page, 'html.parser')\n",
        "\n",
        "            # Получение названия статьи\n",
        "            article_title = soup.find(\"title\").text.strip()\n",
        "\n",
        "            # Получение ссылки на статью\n",
        "            article_link = link\n",
        "\n",
        "            # Получение даты публикации\n",
        "            article_date = soup.find(\"time\")[\"datetime\"]\n",
        "\n",
        "            # Получение ника автора\n",
        "            if soup.find(\"a\", class_=\"tm-user-info__username\"):\n",
        "                article_author = soup.find(\"a\", class_=\"tm-user-info__username\").text.strip()\n",
        "            else:\n",
        "                article_author = \"Автор неизвестен\"\n",
        "\n",
        "            # Получение текста статьи\n",
        "            article_content = soup.find(\"div\", class_=\"article-formatted-body article-formatted-body article-formatted-body_version-2\")\n",
        "\n",
        "            metadata = {\n",
        "                \"author\": article_author,\n",
        "                \"title\": article_title,\n",
        "                \"link\": article_link\n",
        "            }\n",
        "\n",
        "            if article_content:\n",
        "                article_text = [p.text for p in article_content.find_all(\"p\")]\n",
        "\n",
        "                # Сохранение данных в текстовый файл\n",
        "                path = os.path.join(\"corpus\", article_link.split('/')[-2] + \".txt\")\n",
        "                with open(path, \"w+\", encoding=\"utf-8\") as file:\n",
        "                    file.write(json.dumps(metadata, ensure_ascii=False) + \"\\n\\n\")\n",
        "                    file.write(\"\\n\".join(article_text))\n",
        "\n",
        "            # Задержка, чтобы не перегружать сайт\n",
        "            time.sleep(1)\n",
        "\n",
        "        except (requests.exceptions.RequestException, Exception) as e:\n",
        "            print(f\"Error occurred while parsing article {link}: {e}\")\n"
      ],
      "metadata": {
        "id": "ZSVsz2E1y_Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_links = scrape_articles()\n",
        "article_data = parse_article_info(article_links)"
      ],
      "metadata": {
        "id": "BbGhnZnAIPS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_folder = \"corpus\"\n",
        "archive_name = \"corpus.zip\"\n",
        "\n",
        "# Создаем архив\n",
        "with zipfile.ZipFile(archive_name, \"w\") as zip_file:\n",
        "    for root, dirs, files in os.walk(source_folder):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zip_file.write(file_path)\n"
      ],
      "metadata": {
        "id": "4BaK4R-EhFJK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}