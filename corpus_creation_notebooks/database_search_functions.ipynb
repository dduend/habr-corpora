{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozLWXODMPdU8",
        "outputId": "d77c2885-f4f3-42ce-e050-b69f8a7c060d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy3)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3\n",
            "Successfully installed dawg-python-0.7.2 pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import sqlite3\n",
        "from pymorphy3 import MorphAnalyzer"
      ],
      "metadata": {
        "id": "5hwGVNIFAYXg"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph = MorphAnalyzer(lang='ru')"
      ],
      "metadata": {
        "id": "BTd3x9CyYi1w"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = ('NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN', 'INTJ', 'PRON', 'DET', 'NUM', 'CCONJ', 'SCONJ', 'ADP', 'PART', 'AUX')\n",
        "deprel_tags = ('acl', 'acl:relcl', 'advcl', 'advmod', 'amod', 'appos', 'aux',\n",
        "               'aux:pass', 'case', 'cc', 'ccomp', 'conj', 'cop', 'csubj', 'det',\n",
        "               'discourse', 'expl', 'fixed', 'flat', 'flat:name', 'iobj',\n",
        "               'list', 'mark', 'nmod', 'nsubj', 'nsubj:pass', 'nummod',\n",
        "               'nummod:gov', 'obj', 'obl', 'obl:agent', 'parataxis', 'root',\n",
        "               'vocative', 'xcomp')"
      ],
      "metadata": {
        "id": "us5m1n6KbvcT"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_single_token(token):\n",
        "  \"\"\"Эта функция принимает однокомпонентный (без '+') токен\n",
        "        (слово в кавычках/слово без кавычек/POS-тег/deprel-тег) и выдаёт\n",
        "        кортеж, где первый элемент — \"режим поиска\", а второй — элемент, по\n",
        "        которому будет осуществляться поиск.\"\"\"\n",
        "  if token in pos_tags:  # Поиск по части речи\n",
        "    return 'only_pos', token\n",
        "  elif token in deprel_tags:  # Поиск по синтаксической функции\n",
        "    return 'only_deprel', token\n",
        "  else:\n",
        "    if all(char.isascii() for char in token.strip('\"').lower()): # Проверяем,\n",
        "    # что введённое пользователем слово — не полностью написанное латиницей (при\n",
        "    # этом часть символов могут быть латинскими, например \"it-специалиста\" —\n",
        "    # валидный запрос)\n",
        "      print('Слово не может быть полностью написано латиницей! Если же вы имели в виду тег, то среди доступных его нет, проверьте инструкцию по поиску!')\n",
        "      return None\n",
        "    elif re.match(r'^\"(.+)\"$', token):  # Поиск точной формы\n",
        "      return 'exact_wordform', token[1:-1].lower()\n",
        "    else:  # Поиск любых грамматических форм\n",
        "      lemma = morph.normal_forms(token)[0]\n",
        "      return 'lemma', lemma"
      ],
      "metadata": {
        "id": "d_86ikwuz5Wc"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_multipart_token(token):\n",
        "    \"\"\"Эта функция принимает составной (с '+') токен (лемму с тегами — POS\n",
        "        и/или deprel), разбивает по '+' и выдаёт кортеж, где первый элемент —\n",
        "        \"режим поиска\", а последующие — элементы, по которым будет\n",
        "        осуществляться поиск.\"\"\"\n",
        "    parts = token.split('+')\n",
        "    if len(parts) > 3:\n",
        "      print('В токене может быть макс. 3 элемента — лемма, POS-тег и deprel-тег!')\n",
        "      return None\n",
        "    else:\n",
        "      lemma = parts[0].lower()\n",
        "      if len(parts) == 2:\n",
        "        if parts[1] in pos_tags:\n",
        "          return 'lemma+pos', lemma, parts[1]\n",
        "        elif parts[1] in deprel_tags:\n",
        "          return 'lemma+deprel', lemma, parts[1]\n",
        "        else:\n",
        "          print('Введённого тега среди доступных нет, проверьте инструкцию по поиску!')\n",
        "          return None\n",
        "      else:\n",
        "        if parts[1] in pos_tags and parts[2] in deprel_tags:\n",
        "          return 'lemma+pos+deprel', lemma, parts[1], parts[2]\n",
        "        else:\n",
        "          if parts[1] not in pos_tags and parts[2] in deprel_tags:\n",
        "            print('Введённого POS-тега среди доступных нет, проверьте инструкцию по поиску!')\n",
        "            return None\n",
        "          elif parts[1] in pos_tags and parts[2] not in deprel_tags:\n",
        "            print('Введённого deprel-тега среди доступных нет, проверьте инструкцию по поиску!')\n",
        "            return None\n",
        "          elif parts[1] in deprel_tags and parts[2] in pos_tags:\n",
        "            print('Неверный порядок ввода тегов: необходимо ввести сперва POS-, а затем — deprel-тег!')\n",
        "            return None\n",
        "          else:\n",
        "            print('Введённых тегов среди доступных нет, проверьте инструкцию по поиску!')\n",
        "            return None"
      ],
      "metadata": {
        "id": "kh0SwOK4HTJ3"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(user_request):\n",
        "  \"\"\"Эта функция обрабатывает пользовательский запрос и выдаёт список\n",
        "        кортежей для дальнейшего составления запросов к базе данных.\"\"\"\n",
        "  if re.search(r'[^a-zA-Zа-яА-ЯёЁ +:\"]', user_request):\n",
        "    value_error = 'В запросе присутствуют некорректные символы!'\n",
        "    return value_error\n",
        "  else:\n",
        "    tokens = user_request.strip().split()\n",
        "    if len(tokens) > 3:\n",
        "      value_error = 'Длина запроса превышает 3 токена!'\n",
        "      return value_error\n",
        "    else:\n",
        "      parsed_tokens_for_search = []\n",
        "      for token in tokens:\n",
        "        if '+' in token:\n",
        "          result = parse_multipart_token(token)\n",
        "          if isinstance(result, tuple):\n",
        "            parsed_tokens_for_search.append(result)\n",
        "          else:\n",
        "            value_error = result\n",
        "            return value_error\n",
        "        else:\n",
        "          result = parse_single_token(token)\n",
        "          if isinstance(result, tuple):\n",
        "            parsed_tokens_for_search.append(result)\n",
        "          else:\n",
        "            value_error = result\n",
        "            return value_error\n",
        "      return parsed_tokens_for_search"
      ],
      "metadata": {
        "id": "_7HbYgbodtfr"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_user_request = 'компьютер+NOUN+nsubj'\n",
        "correct_user_request_list = process_query(correct_user_request)\n",
        "correct_user_request_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H1vGLLIRCKg",
        "outputId": "41d3639f-8c63-41a8-b429-cc7768418d04"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lemma+pos+deprel', 'компьютер', 'NOUN', 'nsubj')]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_user_request = 'компьютер+nsubj+NOUN'\n",
        "incorrect_user_request_list = process_query('компьютер+nsubj+NOUN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PneWmFzRr4U",
        "outputId": "d2d2df71-4b09-4579-ea26-9fbd1aaf9343"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Неверный порядок ввода тегов: необходимо ввести сперва POS-, а затем — deprel-тег!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(incorrect_user_request_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFwl1Ooefs27",
        "outputId": "f9592ea9-c314-4bb8-89de-3ac574b67fa9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiple_tokens_user_request = 'компьютер+NOUN+nsubj VERB'\n",
        "multiple_tokens_user_request_list = process_query('компьютер+NOUN+nsubj VERB')\n",
        "multiple_tokens_user_request_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8wEL7QIQH_Y",
        "outputId": "94cc2abc-68b6-4963-e384-5811f388062a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lemma+pos+deprel', 'компьютер', 'NOUN', 'nsubj'), ('only_pos', 'VERB')]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('habr_corpus.db')\n",
        "cur = conn.cursor()"
      ],
      "metadata": {
        "id": "tsOLnbbRgnJ7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_exact_wordform(token_info_tuple):\n",
        "    \"\"\"Эта функция принимает кортеж с информацией о токене, составляет\n",
        "        SQL-запрос для поиска по конкретной словоформе и выдаёт список\n",
        "        кортежей из id слов и id предложений из базы данных, подходящих под\n",
        "        запрос.\"\"\"\n",
        "    query_condition = \"\"\"\n",
        "    SELECT word_id, sent_id\n",
        "    FROM words\n",
        "    WHERE token = ?\n",
        "    \"\"\"\n",
        "    cur.execute(query_condition, (token_info_tuple[1],))\n",
        "    output = cur.fetchall()\n",
        "    return output"
      ],
      "metadata": {
        "id": "gUHM6KQKcNje"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_lemma(token_info_tuple):\n",
        "    \"\"\"Эта функция принимает кортеж с информацией о токене, составляет\n",
        "        SQL-запрос для поиска любых грамматических форм и выдаёт список\n",
        "        кортежей из id слов и id предложений из базы данных, подходящих под\n",
        "        запрос.\"\"\"\n",
        "    query_condition = \"\"\"\n",
        "    SELECT word_id, sent_id\n",
        "    FROM words\n",
        "    WHERE lemma = ?\n",
        "    \"\"\"\n",
        "    cur.execute(query_condition, (token_info_tuple[1],))\n",
        "    output = cur.fetchall()\n",
        "    return output"
      ],
      "metadata": {
        "id": "dgsIpcm19O6f"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_only_pos(token_info_tuple):\n",
        "    \"\"\"Эта функция принимает кортеж с информацией о токене, составляет\n",
        "        SQL-запрос для поиска по части речи и выдаёт список кортежей из id\n",
        "        слов и id предложений из базы данных, подходящих под запрос.\"\"\"\n",
        "    query_condition = \"\"\"\n",
        "    SELECT word_id, sent_id\n",
        "    FROM words\n",
        "    WHERE pos = ?\n",
        "    \"\"\"\n",
        "    cur.execute(query_condition, (token_info_tuple[1],))\n",
        "    output = cur.fetchall()\n",
        "    return output"
      ],
      "metadata": {
        "id": "HW2HYyqB-0sn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_only_deprel(token_info_tuple):\n",
        "    \"\"\"Эта функция принимает кортеж с информацией о токене, составляет\n",
        "        SQL-запрос для поиска по синтаксической функции и выдаёт список\n",
        "        кортежей из id слов и id предложений из базы данных, подходящих под\n",
        "        запрос.\"\"\"\n",
        "    query_condition = \"\"\"\n",
        "    SELECT word_id, sent_id\n",
        "    FROM words\n",
        "    WHERE deprel = ?\n",
        "    \"\"\"\n",
        "    cur.execute(query_condition, (token_info_tuple[1],))\n",
        "    output = cur.fetchall()\n",
        "    return output"
      ],
      "metadata": {
        "id": "L94Nzikb_ADG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_lemma_and_pos(token_info_tuple):\n",
        "    \"\"\"Эта функция принимает кортеж с информацией о токене, составляет\n",
        "        SQL-запрос для поиска по лемме со специфицированной частью речи и\n",
        "        выдаёт список кортежей из id слов и id предложений из базы данных,\n",
        "        подходящих под запрос.\"\"\"\n",
        "    query_condition = \"\"\"\n",
        "    SELECT word_id, sent_id\n",
        "    FROM words\n",
        "    WHERE lemma = ? AND pos = ?\n",
        "    \"\"\"\n",
        "    cur.execute(query_condition, (token_info_tuple[1], token_info_tuple[2]))\n",
        "    output = cur.fetchall()\n",
        "    return output"
      ],
      "metadata": {
        "id": "kJ6HNIbv_PSd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_lemma_and_deprel(token_info_tuple):\n",
        "    \"\"\"Эта функция принимает кортеж с информацией о токене, составляет\n",
        "        SQL-запрос для поиска по лемме со специфицированной синтаксической\n",
        "        функцией и выдаёт список кортежей из id слов и id предложений из базы\n",
        "        данных, подходящих под запрос.\"\"\"\n",
        "    query_condition = \"\"\"\n",
        "    SELECT word_id, sent_id\n",
        "    FROM words\n",
        "    WHERE lemma = ? AND deprel = ?\n",
        "    \"\"\"\n",
        "    cur.execute(query_condition, (token_info_tuple[1], token_info_tuple[2]))\n",
        "    output = cur.fetchall()\n",
        "    return output"
      ],
      "metadata": {
        "id": "TRp5Dria_lOz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_lemma_and_pos_and_deprel(token_info_tuple):\n",
        "    \"\"\"Эта функция принимает кортеж с информацией о токене, составляет\n",
        "        SQL-запрос для поиска по лемме со специфицированными частью речи И\n",
        "        синтаксической функцией и выдаёт список кортежей из id слов и id\n",
        "        предложений из базы данных, подходящих под запрос.\"\"\"\n",
        "    query_condition = \"\"\"\n",
        "    SELECT word_id, sent_id\n",
        "    FROM words\n",
        "    WHERE lemma = ? AND pos = ? AND deprel = ?\n",
        "    \"\"\"\n",
        "    cur.execute(query_condition, (token_info_tuple[1], token_info_tuple[2], token_info_tuple[3]))\n",
        "    output = cur.fetchall()\n",
        "    return output"
      ],
      "metadata": {
        "id": "M9B9_o_A_rTn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_example = search_lemma_and_pos_and_deprel(('lemma+pos+deprel', 'компьютер', 'NOUN', 'nsubj'))\n",
        "output_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRjdXKzmQdf-",
        "outputId": "355c0ab7-2cf9-41d7-d7bb-dc19ee4adacb"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(35488, 2506),\n",
              " (79850, 5899),\n",
              " (100765, 7655),\n",
              " (161361, 12169),\n",
              " (169720, 12722),\n",
              " (179111, 13410),\n",
              " (212291, 15898)]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_for_next_token(output, next_token_info_tuple):\n",
        "    \"\"\"Эта функция принимает список кортежей из id слов и id предложений,\n",
        "        в которых содержится первый токен, из базы данных, а также кортеж с\n",
        "        информацией о следующем токене и выдаёт список кортежей из id слов и\n",
        "        id предложений, в которых содержатся первый и следующий токены, из\n",
        "        базы данных.\"\"\"\n",
        "    next_token_output = []\n",
        "    if next_token_info_tuple[0] == 'exact_wordform':\n",
        "        element = 'token'\n",
        "    elif next_token_info_tuple[0] == 'only_pos':\n",
        "        element = 'pos'\n",
        "    elif next_token_info_tuple[0] == 'only_deprel':\n",
        "        element = 'deprel'\n",
        "    else:\n",
        "        element = 'lemma'\n",
        "        if next_token_info_tuple[0] == 'lemma+pos':\n",
        "            element += ' = ? AND pos'\n",
        "        elif next_token_info_tuple[0] == 'lemma+deprel':\n",
        "            element += ' = ? AND deprel'\n",
        "        elif next_token_info_tuple[0] == 'lemma+pos+deprel':\n",
        "            element += ' = ? AND pos = ? AND deprel'\n",
        "    next_token_query_condition = f\"\"\"\n",
        "    SELECT word_id, sent_id\n",
        "    FROM words\n",
        "    WHERE {element} = ? AND word_id = ? AND sent_id = ?\n",
        "    \"\"\"\n",
        "    for word_id, sent_id in output:\n",
        "        next_word_id = word_id + 1\n",
        "        if ' ' in element:\n",
        "            elements = element.split(' = ? AND ')\n",
        "            if len(elements) == 2:\n",
        "                cur.execute(next_token_query_condition, (next_token_info_tuple[1], next_token_info_tuple[2], next_word_id, sent_id))\n",
        "            else:\n",
        "                cur.execute(next_token_query_condition, (next_token_info_tuple[1], next_token_info_tuple[2], next_token_info_tuple[3], next_word_id, sent_id))\n",
        "        else:\n",
        "            cur.execute(next_token_query_condition, (next_token_info_tuple[1], next_word_id, sent_id))\n",
        "        result = cur.fetchone()\n",
        "        if result:\n",
        "            next_token_output.append(result)\n",
        "    return next_token_output"
      ],
      "metadata": {
        "id": "NX__NXy6_7eu"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_output_for_next_token(output_example, ('only_pos', 'VERB'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHbCSUGYQkL5",
        "outputId": "b307b47c-70c9-4c1c-e384-adf848bc12b2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(79851, 5899)]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences_idxs(parsed_tokens_for_search):\n",
        "    \"\"\"Эта функция принимает список кортежей с разбором 1-3 токенов, с\n",
        "        помощью функций выше преобразует в SQL-запросы к базе данных и выдаёт\n",
        "        список id предложений, подходящих под пользовательский запрос.\"\"\"\n",
        "    if parsed_tokens_for_search[0][0] == 'exact_wordform':\n",
        "        output = search_exact_wordform(parsed_tokens_for_search[0])\n",
        "    elif parsed_tokens_for_search[0][0] == 'lemma':\n",
        "        output = search_lemma(parsed_tokens_for_search[0])\n",
        "    elif parsed_tokens_for_search[0][0] == 'only_pos':\n",
        "        output = search_only_pos(parsed_tokens_for_search[0])\n",
        "    elif parsed_tokens_for_search[0][0] == 'only_deprel':\n",
        "        output = search_only_deprel(parsed_tokens_for_search[0])\n",
        "    elif parsed_tokens_for_search[0][0] == 'lemma+pos':\n",
        "        output = search_lemma_and_pos(parsed_tokens_for_search[0])\n",
        "    elif parsed_tokens_for_search[0][0] == 'lemma+deprel':\n",
        "        output = search_lemma_and_deprel(parsed_tokens_for_search[0])\n",
        "    else:\n",
        "        output = search_lemma_and_pos_and_deprel(parsed_tokens_for_search[0])\n",
        "\n",
        "    if output and len(parsed_tokens_for_search) > 1:\n",
        "        next_token_output = get_output_for_next_token(output, parsed_tokens_for_search[1])\n",
        "        if len(parsed_tokens_for_search) == 3:\n",
        "            next_token_output = get_output_for_next_token(next_token_output, parsed_tokens_for_search[2])\n",
        "            sentences_idxs = [result[1] for result in next_token_output]\n",
        "            return sentences_idxs\n",
        "        else:\n",
        "            sentences_idxs = [result[1] for result in next_token_output]\n",
        "            return sentences_idxs  # получаем список айдишников предложений\n",
        "    if output:\n",
        "        output = [result[1] for result in output]\n",
        "    return output"
      ],
      "metadata": {
        "id": "UAA49OgIHpsJ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_sentences_idxs(correct_user_request_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmyZJSS9KZT2",
        "outputId": "5c4d3639-e81b-45e9-f4a4-2706df7f8e64"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2506, 5899, 7655, 12169, 12722, 13410, 15898]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = get_sentences_idxs(correct_user_request_list)"
      ],
      "metadata": {
        "id": "paFR7gAiL8QE"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multiple_tokens_user_request_idxs = get_sentences_idxs(multiple_tokens_user_request_list)"
      ],
      "metadata": {
        "id": "QY2GM0KhShTV"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences_text_with_metainfo(idxs):\n",
        "    \"\"\"Эта функция принимает id предложений, подходящих под\n",
        "        пользовательский запрос, составляет SQL-запрос с использованием JOIN\n",
        "        для объединения данных из таблиц sentences и articles базы данных и\n",
        "        выдаёт предложения с метаинформацией (автор, заголовок, ссылка на\n",
        "        статью).\"\"\"\n",
        "    if len(idxs) == 1:\n",
        "        idxs = f'({idxs[0]})'\n",
        "    else:\n",
        "        idxs = tuple(idxs)\n",
        "    sentences_with_metainfo_query_condition = f'''\n",
        "        SELECT DISTINCT sentence, author, title, link FROM sentences\n",
        "        JOIN articles ON sentences.article_id=articles.article_id\n",
        "        WHERE sentences.sent_id in {idxs}\n",
        "    '''\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(sentences_with_metainfo_query_condition)\n",
        "    sentences_with_metainfo = cur.fetchall()\n",
        "    return sentences_with_metainfo"
      ],
      "metadata": {
        "id": "b0l3cP3WMJpW"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentences_text_with_metainfo(idxs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeGE63m2N-um",
        "outputId": "4f6f7dd8-6447-48a7-8f66-887bc22f5029"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Представьте себе 1970-е годы, время, когда компьютеры только начинали проникать в повседневную жизнь.',\n",
              "  'gnatyuk_sergey',\n",
              "  'Objective-C не кусается: как перестать бояться Legacy и стать настоящим iOS-ниндзя / Хабр',\n",
              "  'https://habr.com//ru/articles/848788/'),\n",
              " ('С такими людьми компьютеры становятся вдвойне веселее.',\n",
              "  'spring_aio',\n",
              "  'Взлом JVM-приложения с помощью отладчика IntelliJ IDEA / Хабр',\n",
              "  'https://habr.com//ru/companies/spring_aio/articles/845554/'),\n",
              " ('Объективно, не у всех ребят очень мощные компьютеры.',\n",
              "  'Aeliot',\n",
              "  'Автоматизация CQC на CI / Хабр',\n",
              "  'https://habr.com//ru/articles/852978/'),\n",
              " ('И хотя для практического применения Q-Newton потребуются достаточно мощные квантовые компьютеры, подобные гибридные схемы имеют все шансы преодолеть вычислительные ограничения классических систем.',\n",
              "  'breakmirrors',\n",
              "  '«А можно быстрее?»: практические советы по ускорению обучения нейросетей / Хабр',\n",
              "  'https://habr.com//ru/companies/magnus-tech/articles/846012/'),\n",
              " ('Жак Арсак - профессор и преподаватель программирования из Франции, из времён когда компьютеры были большими а их возможности - очень скромными :)',\n",
              "  'RodionGork',\n",
              "  'По следам Жака Арсака — о программировании игр / Хабр',\n",
              "  'https://habr.com//ru/articles/850764/'),\n",
              " ('MicroText создавался в начале 80-х годов, когда персональные компьютеры только начинали завоёвывать популярность.',\n",
              "  'Cloud4Y',\n",
              "  'Гик-блогер возродил почти забытый язык программирования для Commodore / Хабр',\n",
              "  'https://habr.com//ru/companies/cloud4y/articles/850696/'),\n",
              " ('Здоровенный том на 1000+ страниц, местами очень и очень непростой, но позволяет понять, как работает компьютер в целом.',\n",
              "  'AQUARIUS_1989',\n",
              "  'Настольная библиотека HDL-дизайнера и верификатора / Хабр',\n",
              "  'https://habr.com//ru/companies/aquarius/articles/848334/')]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentences_text_with_metainfo(multiple_tokens_user_request_idxs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szp3U9OUS1w_",
        "outputId": "f2734147-dca0-49b6-f420-9ae7fa9b3e9b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('С такими людьми компьютеры становятся вдвойне веселее.',\n",
              "  'spring_aio',\n",
              "  'Взлом JVM-приложения с помощью отладчика IntelliJ IDEA / Хабр',\n",
              "  'https://habr.com//ru/companies/spring_aio/articles/845554/')]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ]
}